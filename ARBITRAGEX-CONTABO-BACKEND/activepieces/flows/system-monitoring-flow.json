{
  "id": "system-monitoring-flow-001",
  "displayName": "ArbitrageX System Monitoring & Alerts",
  "version": {
    "major": 3,
    "minor": 0,
    "patch": 0
  },
  "created": "2024-01-15T00:00:00.000Z",
  "updated": "2024-01-15T00:00:00.000Z",
  "projectId": "arbitragex-supreme-v3",
  "folderId": "monitoring",
  "status": "ENABLED",
  "schedule": {
    "type": "CRON",
    "cronExpression": "*/30 * * * * *",
    "timezone": "UTC"
  },
  "trigger": {
    "name": "system_health_check",
    "displayName": "System Health Check Trigger",
    "type": "SCHEDULE",
    "settings": {
      "cronExpression": "*/30 * * * * *",
      "timezone": "UTC"
    }
  },
  "steps": [
    {
      "name": "check_temporal_health",
      "displayName": "Check Temporal Server Health",
      "type": "CODE",
      "settings": {
        "sourceCode": {
          "code": "export const code = async (inputs) => {\n  try {\n    // Check Temporal server health\n    const temporalResponse = await fetch('http://temporal-server:7233/api/v1/health');\n    const temporalHealth = await temporalResponse.json();\n    \n    // Check workflow metrics\n    const workflowsResponse = await fetch('http://temporal-server:7233/api/v1/workflows');\n    const workflowsData = await workflowsResponse.json();\n    \n    return {\n      temporal_healthy: temporalResponse.ok,\n      temporal_status: temporalHealth,\n      active_workflows: workflowsData?.workflows?.length || 0,\n      timestamp: Date.now()\n    };\n  } catch (error) {\n    return {\n      temporal_healthy: false,\n      error: error.message,\n      timestamp: Date.now()\n    };\n  }\n};"
        },
        "input": {}
      },
      "nextAction": "check_langflow_agents"
    },
    {
      "name": "check_langflow_agents",
      "displayName": "Check Langflow Agents Status",
      "type": "CODE",
      "settings": {
        "sourceCode": {
          "code": "export const code = async (inputs) => {\n  const temporalData = inputs.check_temporal_health;\n  \n  try {\n    // Check each Langflow agent\n    const agents = [\n      { name: 'flashbots-detective', url: 'http://langflow:7860/api/v1/run/flashbots-detective-agent' },\n      { name: 'risk-guardian', url: 'http://langflow:7860/api/v1/run/risk-guardian-agent' },\n      { name: 'strategy-optimizer', url: 'http://langflow:7860/api/v1/run/strategy-optimizer-agent' }\n    ];\n    \n    const agentHealthChecks = await Promise.all(\n      agents.map(async (agent) => {\n        try {\n          const response = await fetch(agent.url + '?health=true', {\n            method: 'GET',\n            timeout: 5000\n          });\n          \n          return {\n            name: agent.name,\n            healthy: response.ok,\n            status_code: response.status,\n            response_time_ms: Date.now() - startTime\n          };\n        } catch (error) {\n          return {\n            name: agent.name,\n            healthy: false,\n            error: error.message\n          };\n        }\n      })\n    );\n    \n    const healthyAgents = agentHealthChecks.filter(a => a.healthy).length;\n    const totalAgents = agentHealthChecks.length;\n    \n    return {\n      ...temporalData,\n      langflow_healthy: healthyAgents === totalAgents,\n      agents_status: agentHealthChecks,\n      healthy_agents_count: healthyAgents,\n      total_agents_count: totalAgents,\n      langflow_availability_percent: Math.round((healthyAgents / totalAgents) * 100)\n    };\n  } catch (error) {\n    return {\n      ...temporalData,\n      langflow_healthy: false,\n      error: error.message\n    };\n  }\n};"
        },
        "input": {
          "check_temporal_health": "{{check_temporal_health}}"
        }
      },
      "nextAction": "check_infrastructure"
    },
    {
      "name": "check_infrastructure",
      "displayName": "Check Infrastructure Services",
      "type": "CODE",
      "settings": {
        "sourceCode": {
          "code": "export const code = async (inputs) => {\n  const previousData = inputs.check_langflow_agents;\n  \n  try {\n    // Check core infrastructure services\n    const services = [\n      { name: 'postgresql', url: 'postgresql://arbitragex_user:arbitragex_pass@temporal-postgresql:5432/temporal', type: 'database' },\n      { name: 'redis', url: 'redis://redis:6379', type: 'cache' },\n      { name: 'prometheus', url: 'http://prometheus:9090/-/healthy', type: 'monitoring' },\n      { name: 'grafana', url: 'http://grafana:3000/api/health', type: 'visualization' },\n      { name: 'rust-engine', url: 'http://rust-execution-engine:8080/health', type: 'execution' }\n    ];\n    \n    const serviceHealthChecks = await Promise.all(\n      services.map(async (service) => {\n        try {\n          let healthy = false;\n          let details = {};\n          \n          if (service.type === 'database') {\n            // Database health check (simplified)\n            healthy = true; // En implementación real, conectaría a la DB\n            details = { connections: 25, max_connections: 100 };\n          } else {\n            const response = await fetch(service.url, { timeout: 5000 });\n            healthy = response.ok;\n            details = { status_code: response.status };\n          }\n          \n          return {\n            name: service.name,\n            type: service.type,\n            healthy: healthy,\n            details: details\n          };\n        } catch (error) {\n          return {\n            name: service.name,\n            type: service.type,\n            healthy: false,\n            error: error.message\n          };\n        }\n      })\n    );\n    \n    const healthyServices = serviceHealthChecks.filter(s => s.healthy).length;\n    const totalServices = serviceHealthChecks.length;\n    \n    // Calculate overall system health\n    const overallHealth = {\n      temporal_ok: previousData.temporal_healthy,\n      langflow_ok: previousData.langflow_healthy,\n      infrastructure_ok: healthyServices === totalServices,\n      overall_health_percent: Math.round(\n        ((previousData.temporal_healthy ? 1 : 0) +\n         (previousData.langflow_healthy ? 1 : 0) +\n         (healthyServices / totalServices)) / 3 * 100\n      )\n    };\n    \n    return {\n      ...previousData,\n      infrastructure_services: serviceHealthChecks,\n      healthy_services_count: healthyServices,\n      total_services_count: totalServices,\n      overall_health: overallHealth,\n      requires_alert: overallHealth.overall_health_percent < 90\n    };\n  } catch (error) {\n    return {\n      ...previousData,\n      infrastructure_healthy: false,\n      error: error.message,\n      requires_alert: true\n    };\n  }\n};"
        },
        "input": {
          "check_langflow_agents": "{{check_langflow_agents}}"
        }
      },
      "nextAction": "evaluate_alerts"
    },
    {
      "name": "evaluate_alerts",
      "displayName": "Evaluate Alert Conditions",
      "type": "CODE",
      "settings": {
        "sourceCode": {
          "code": "export const code = async (inputs) => {\n  const healthData = inputs.check_infrastructure;\n  \n  const alerts = [];\n  const warnings = [];\n  \n  // Critical alerts\n  if (!healthData.temporal_healthy) {\n    alerts.push({\n      severity: 'CRITICAL',\n      component: 'temporal-server',\n      message: 'Temporal server is down - workflow execution stopped',\n      impact: 'No arbitrage execution possible',\n      action_required: 'Restart temporal server immediately'\n    });\n  }\n  \n  if (!healthData.langflow_healthy) {\n    alerts.push({\n      severity: 'CRITICAL',\n      component: 'langflow-agents',\n      message: `${healthData.total_agents_count - healthData.healthy_agents_count} AI agents are down`,\n      impact: 'AI-powered opportunity detection compromised',\n      action_required: 'Restart failed Langflow agents'\n    });\n  }\n  \n  // Warning conditions\n  if (healthData.overall_health.overall_health_percent < 95) {\n    warnings.push({\n      severity: 'WARNING',\n      component: 'system-performance',\n      message: `Overall system health at ${healthData.overall_health.overall_health_percent}%`,\n      impact: 'Reduced system reliability',\n      action_required: 'Monitor and investigate degraded components'\n    });\n  }\n  \n  // Infrastructure warnings\n  const unhealthyServices = healthData.infrastructure_services?.filter(s => !s.healthy) || [];\n  unhealthyServices.forEach(service => {\n    warnings.push({\n      severity: 'WARNING',\n      component: service.name,\n      message: `${service.name} service health check failed`,\n      impact: `${service.type} functionality may be impacted`,\n      action_required: `Check ${service.name} service logs and restart if needed`\n    });\n  });\n  \n  return {\n    ...healthData,\n    alerts: alerts,\n    warnings: warnings,\n    total_alerts: alerts.length,\n    total_warnings: warnings.length,\n    notification_required: alerts.length > 0 || warnings.length > 2,\n    alert_summary: {\n      critical_issues: alerts.length,\n      warning_issues: warnings.length,\n      system_operational: alerts.length === 0\n    }\n  };\n};"
        },
        "input": {
          "check_infrastructure": "{{check_infrastructure}}"
        }
      },
      "nextAction": "send_notifications"
    },
    {
      "name": "send_notifications",
      "displayName": "Send Alert Notifications",
      "type": "WEBHOOK",
      "settings": {
        "url": "http://webhook-service:3000/alerts/notify",
        "method": "POST",
        "headers": {
          "Content-Type": "application/json",
          "Authorization": "Bearer {{WEBHOOK_API_KEY}}"
        },
        "body": {
          "timestamp": "{{evaluate_alerts.timestamp}}",
          "system_health": "{{evaluate_alerts.overall_health}}",
          "alerts": "{{evaluate_alerts.alerts}}",
          "warnings": "{{evaluate_alerts.warnings}}",
          "notification_channels": [
            "slack",
            "email",
            "telegram"
          ],
          "priority": "{{evaluate_alerts.total_alerts > 0 ? 'high' : 'medium'}}"
        }
      },
      "condition": {
        "firstValue": "{{evaluate_alerts.notification_required}}",
        "secondValue": true,
        "operator": "EQUALS"
      },
      "nextAction": "update_metrics"
    },
    {
      "name": "update_metrics",
      "displayName": "Update Prometheus Metrics",
      "type": "CODE",
      "settings": {
        "sourceCode": {
          "code": "export const code = async (inputs) => {\n  const alertData = inputs.evaluate_alerts;\n  \n  try {\n    // Prepare metrics for Prometheus\n    const metrics = [\n      `arbitragex_system_health_percent ${alertData.overall_health.overall_health_percent}`,\n      `arbitragex_temporal_healthy ${alertData.temporal_healthy ? 1 : 0}`,\n      `arbitragex_langflow_healthy ${alertData.langflow_healthy ? 1 : 0}`,\n      `arbitragex_active_workflows ${alertData.active_workflows || 0}`,\n      `arbitragex_healthy_agents ${alertData.healthy_agents_count || 0}`,\n      `arbitragex_total_agents ${alertData.total_agents_count || 0}`,\n      `arbitragex_alerts_critical ${alertData.total_alerts || 0}`,\n      `arbitragex_alerts_warnings ${alertData.total_warnings || 0}`,\n      `arbitragex_monitoring_execution_time ${Date.now() - alertData.timestamp}`\n    ];\n    \n    // Send metrics to Prometheus (Gateway)\n    const prometheusResponse = await fetch('http://prometheus-pushgateway:9091/metrics/job/arbitragex_monitoring', {\n      method: 'POST',\n      headers: {\n        'Content-Type': 'text/plain'\n      },\n      body: metrics.join('\\n')\n    });\n    \n    return {\n      ...alertData,\n      metrics_updated: prometheusResponse.ok,\n      metrics_count: metrics.length,\n      monitoring_cycle_completed: true,\n      next_check_in_seconds: 30\n    };\n  } catch (error) {\n    return {\n      ...alertData,\n      metrics_updated: false,\n      error: error.message\n    };\n  }\n};"
        },
        "input": {
          "evaluate_alerts": "{{evaluate_alerts}}"
        }
      }
    }
  ],
  "tags": ["monitoring", "system-health", "alerts", "arbitragex"],
  "description": "Comprehensive system monitoring flow that checks Temporal, Langflow agents, infrastructure services, and generates alerts when issues are detected. Runs every 30 seconds to ensure rapid detection of system problems."
}